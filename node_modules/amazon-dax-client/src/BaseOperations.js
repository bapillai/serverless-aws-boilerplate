/*
 * Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"). You may not
 * use this file except in compliance with the License. A copy of the License
 * is located at
 *
 *    http://aws.amazon.com/apache2.0/
 *
 * or in the "license" file accompanying this file. This file is distributed on
 * an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
 * express or implied. See the License for the specific language governing
 * permissions and limitations under the License.
 */
'use strict';
const DaxClientError = require('./DaxClientError');
const DaxErrorCode = require('./DaxErrorCode');
const StreamBuffer = require('./ByteStreamBuffer');
const CborEncoder = require('./CborEncoder');
const Encoder = require('./Encoders');
const AttributeValueEncoder = require('./AttributeValueEncoder');
const Constants = require('./Constants');
const Util = require('./Util');
const DaxServiceError = require('./DaxServiceError');
const Tube = require('./Tube');

const MAX_WRITE_BATCH_SIZE = 25;
const MAX_READ_BATCH_SIZE = 100;
const BATCH_WRITE_MAX_ITEM_SIZE = 409600;

module.exports = class BaseOperations {
  constructor(keyCache, attrListCache, attrListIdCache, tubePool, requestTimeout) {
    this.tubePool = tubePool;

    this.keyCache = keyCache;
    this.attrListCache = attrListCache;
    this.attrListIdCache = attrListIdCache;
    this.caches = {
      attrListCache: this.attrListCache,
      attrListIdCache: this.attrListIdCache,
      keyCache: this.keyCache,
    };

    this._requestTimeout = requestTimeout || 0;
  }

  _getReturnHandler(tube, assembler, name) {
    return new Promise((resolve, reject) => {
      tube.socket.on('data', (data) => {
        let result;
        try {
          // Pass data to the assembler
          result = assembler.feed(data);
        } catch(err) {
          // Catch & reject any errors, including those returned from the server
          if(err._tubeInvalid) {
            tube.invalidateAuth();
          }

          if(!(err instanceof DaxServiceError)) {
            // On non-DAX errors, reset the pool
            this.tubePool.reset(tube);
          } else {
            // On DAX errors, the tube is still usable
            this.tubePool.recycle(tube);
          }
          return reject(err);
        }

        if(result) {
          // If the response is complete, resolve immediately
          return resolve(result);
        }

        // Otherwise, the assembler needs more data, so wait for it
      });

      tube.socket.on('error', (err) => {
        // On socket errors, reset the pool
        this.tubePool.reset(tube);

        return reject(err);
      });

      let timeout = this._requestTimeout; // capture the timeout in case it changes
      tube.setTimeout(timeout, () => {
        // Either the network is down or the node is stuck. Either way, the pool can be reset.
        this.tubePool.reset(tube);
        return reject(new Tube.TimeoutError(timeout));
      });
    })
    .then((result) => {
      this.tubePool.recycle(tube);
      return this._resolveAttributeValues(result);
    });
  }

  _resolveAttributeValues(response) {
    let deanonymizePromiseList = [];
    if(response.Item) {
      deanonymizePromiseList.push(
        BaseOperations._resolveItemAttributeValues(this.attrListCache, response.Item)
      );
    }

    if(response.Attributes) {
      deanonymizePromiseList.push(
        BaseOperations._resolveItemAttributeValues(this.attrListCache, response.Attributes)
      );
    }

    if(response.Items) {
      for(let item of response.Items) {
        deanonymizePromiseList.push(
          BaseOperations._resolveItemAttributeValues(this.attrListCache, item)
        );
      }
    }

    if(response.Responses) {
      for(let tableName in response.Responses) {
        if(response.Responses.hasOwnProperty(tableName)) {
          let tableResults = response.Responses[tableName];
          for(let item of tableResults) {
            deanonymizePromiseList.push(
              BaseOperations._resolveItemAttributeValues(this.attrListCache, item)
            );
          }
        }
      }
    }

    if(response.UnprocessedItems) {
      for(let tableName in response.UnprocessedItems) {
        if(response.UnprocessedItems.hasOwnProperty(tableName)) {
          let unprocessedRequests = response.UnprocessedItems[tableName];
          for(let unprocessedRequest of unprocessedRequests) {
            for(let i = 0; i < unprocessedRequest.length; i++) {
              let writeType = unprocessedRequest[i][0];
              let writeRequest = unprocessedRequest[i][0];
              if(writeType === 'PutRequest' && writeRequest.Item) {
                deanonymizePromiseList.push(
                  BaseOperations._resolveItemAttributeValues(this.attrListCache, writeRequest.Item)
                );
              }
            }
          }
        }
      }
    }

    return Promise.all(deanonymizePromiseList).then(() => response);
  }

  static _resolveItemAttributeValues(attrListCache, item) {
    if(item && item._attrListId !== undefined) {
      return attrListCache.get(item._attrListId).then((attrNames) => {
        Util.deanonymizeAttributeValues(item, attrNames);
      });
    } else {
      return Promise.resolve(item);
    }
  }

  _validateBatchGetItem(request) {
    if(!request.RequestItems) {
      throw new DaxClientError('1 validation error detected: Value ' +
        JSON.stringify(request.RequestItems) +
        ' at "requestItems" failed to satisfy constraint: Member must have length greater than or equal to 1',
        DaxErrorCode.Validation, false);
    }
    let requestByTable = request.RequestItems;
    let batchSize = 0;
    let isEmpty = true;

    Object.keys(requestByTable).forEach((tableName) => {
      let kaas = requestByTable[tableName];
      if(!kaas) {
        throw new DaxClientError('Request can not be null for table ' + tableName, DaxErrorCode.Validation, false);
      }
      if(!Object.keys(kaas).length) {
        throw new DaxClientError('Keys can not be null for table ' + tableName, DaxErrorCode.Validation, false);
      }
      batchSize += Object.keys(kaas).length;
      if(batchSize > MAX_READ_BATCH_SIZE) {
        throw new DaxClientError('Batch size should be less than ' + MAX_READ_BATCH_SIZE, DaxErrorCode.Validation, false);
      }
      isEmpty = false;
    });

    if(isEmpty) {
      throw new DaxClientError(
        '1 validation error detected: Value null at "requestItems" failed to satisfy constraint: Member must not be null',
        DaxErrorCode.Validation, false);
    }
  }

  prepare_batchGetItem_N697851100_1(request) {
    this._validateBatchGetItem(request);
    let stubData = {};
    let requestByTable = request.RequestItems;
    let keysPerTable = {};
    let tableProjOrdinals = {};
    let buffer = new StreamBuffer();
    let encoder = new CborEncoder();

    buffer.write(encoder.encodeMapHeader(Object.keys(requestByTable).length));

    let fetchKeySchema = [];
    Object.keys(requestByTable).forEach((tableName) => {
      fetchKeySchema.push(this.keyCache.get(tableName).then((keySchema) => {
        keysPerTable[tableName] = keySchema;
      }));
    });

    return Promise.all(fetchKeySchema).then(() => {
      let keySet = new Set();
      Object.keys(requestByTable).forEach((tableName) => {
        keySet.clear();
        let kaas = requestByTable[tableName];

        buffer.write(encoder.encodeString(tableName));
        buffer.write(encoder.encodeArrayHeader(3));

        if(!kaas.ConsistentRead) {
          buffer.write(encoder.encodeBoolean(false));
        } else {
          buffer.write(encoder.encodeBoolean(true));
        }

        tableProjOrdinals[tableName] = [];
        if(kaas.ProjectionExpression) {
          buffer.write(encoder.encodeBinary(Encoder._encodeProjection(kaas.ProjectionExpression, kaas.ExpressionAttributeNames)));
          Encoder._prepareProjection(kaas.ProjectionExpression, kaas.ExpressionAttributeNames, tableProjOrdinals[tableName]);
        } else {
          buffer.write(encoder.encodeNull());
        }


        let keys = (kaas.Keys ? kaas.Keys : []);

        buffer.write(encoder.encodeArrayHeader(keys.length));
        for(let key of keys) {
          let keyBytes = Encoder.encodeKey(key, keysPerTable[tableName], encoder);
          if(keySet.has(keyBytes)) {
            throw new DaxClientError('Provided list of item keys contains duplicates', DaxErrorCode.Validation, false);
          }
          keySet.add(keyBytes);
          buffer.write(keyBytes);
        }
      });

      stubData.getItemKeys = buffer.read();
      let hasKwargs = request.ReturnConsumedCapacity;
      if(hasKwargs) {
        buffer.write(encoder.encodeMapStreamHeader());
        if(request.ReturnConsumedCapacity) {
          buffer.write(encoder.encodeInt(Constants.DaxDataRequestParam.ReturnConsumedCapacity));
          buffer.write(encoder.encodeInt(Constants.ReturnConsumedCapacityValues[request.ReturnConsumedCapacity]));
        }

        buffer.write(encoder.encodeStreamBreak());
      } else {
        buffer.write(encoder.encodeNull());
      }

      stubData.kwargs = buffer.read();
      // attach data neccessary for decoding to request
      request._keysPerTable = keysPerTable;
      request._tableProjOrdinals = tableProjOrdinals;
      request._attrListCache = this.attrListCache; // need cache for assemble, it's more efficient during that phase
      
      request._stubData = stubData;
      return stubData;
    });
  }

  write_batchGetItem_N697851100_1(data, tube) {
    tube.write(tube.cbor.encodeInt(1));
    tube.write(tube.cbor.encodeInt(-697851100));
    tube.write(data.getItemKeys ? data.getItemKeys : tube.cbor.encodeNull());
    tube.write(data.kwargs ? data.kwargs : tube.cbor.encodeNull());
    tube.flush();
  }

  prepare_batchWriteItem_116217951_1(request) {
    let stubData = {};
    let keysPerTable = {};
    let attrListIdPerTable = {};
    let requestByTable = request.RequestItems;
    let buffer = new StreamBuffer();
    let encoder = new CborEncoder();
    let totalRequests = 0;
    if(!requestByTable) {
      throw new DaxClientError('1 validation error detected: Value ' +
        JSON.stringify(request.RequestItems) +
        ' at "requestItems" failed to satisfy constraint: Member must have length greater than or equal to 1',
        DaxErrorCode.Validation, false);
    }

    let fetchKeySchema = [];
    Object.keys(requestByTable).forEach((tableName) => {
      fetchKeySchema.push(this.keyCache.get(tableName).then((keySchema) => {
        keysPerTable[tableName] = keySchema;
      }));
    });

    let fetchAttributeSchema = [];

    buffer.write(encoder.encodeMapHeader(Object.keys(requestByTable).length));

    return Promise.all(fetchKeySchema).then(() => {
      Object.keys(requestByTable).forEach((tableName) => {
        attrListIdPerTable[tableName] = [];
        for(let i = 0; i < requestByTable[tableName].length; ++i) {
          if(requestByTable[tableName][i].PutRequest) {
            let attrNames = AttributeValueEncoder.getCanonicalAttributeList(requestByTable[tableName][i].PutRequest.Item, keysPerTable[tableName]);
            fetchAttributeSchema.push(this.attrListIdCache.get(attrNames).then((attrListId) => {
              attrListIdPerTable[tableName][i] = attrListId;
            }));
          }
        }
      });

      return Promise.all(fetchAttributeSchema);
    }).then(() => {
      let keySet = new Set();
      Object.keys(requestByTable).forEach((tableName) => {
        keySet.clear();
        if(!tableName) {
          throw new DaxClientError('Value null at "tableName" failed to satisfy constraint: Member must not be null', DaxErrorCode.Validation, false);
        }

        let writeRequests = requestByTable[tableName];
        if((totalRequests += writeRequests.length) > MAX_WRITE_BATCH_SIZE) {
          throw new DaxClientError(`Batch size should be less than ${MAX_WRITE_BATCH_SIZE}`, DaxErrorCode.Validation, false);
        }

        buffer.write(encoder.encodeString(tableName));

        let requestItemCount = 0;
        let isEmpty = true;
        for(let writeRequest of writeRequests) {
          if(writeRequest.PutRequest || writeRequest.DeleteRequest) {
            requestItemCount++;
          }
        }
        buffer.write(encoder.encodeArrayHeader(requestItemCount * 2));

        for(let i = 0; i < writeRequests.length; ++i) {
          let writeRequest = writeRequests[i];
          if(!writeRequest) {
            continue;
          }

          isEmpty = false;
          this._validateWriteRequest(writeRequest);
          if(writeRequest.PutRequest) {
            let attributes = writeRequest.PutRequest.Item;
            this._validateBatchWriteItem(attributes);
            let keyBytes = Encoder.encodeKey(attributes, keysPerTable[tableName], encoder);
            if(keySet.has(keyBytes)) {
              throw new DaxClientError('Provided list of item keys contains duplicates', DaxErrorCode.Validation, false);
            }
            keySet.add(keyBytes);
            buffer.write(keyBytes);
            buffer.write(Encoder.encodeValuesWithKeys(attributes, keysPerTable[tableName], attrListIdPerTable[tableName][i], encoder));
          } else if(writeRequest.DeleteRequest) {
            let key = writeRequest.DeleteRequest.Key;
            let keyBytes = Encoder.encodeKey(key, keysPerTable[tableName], encoder);
            if(keySet.has(keyBytes)) {
              throw new DaxClientError('Provided list of item keys contains duplicates', DaxErrorCode.Validation, false);
            }
            keySet.add(keyBytes);
            buffer.write(keyBytes);
            buffer.write(encoder.encodeNull());
          }
        }
        if(isEmpty) {
          throw new DaxClientError(`1 validation error detected: Value '{ ${tableName} =`,
            JSON.stringify(writeRequests),
            `}' at 'requestItems' failed to satisfy constraint: Map value must satisfy constraint:`,
            `[Member must have length less than or equal to 25, Member must have length greater than or equal to 1`,
            DaxErrorCode.Validation, false);
        }
      });

      if(totalRequests === 0) {
        throw new DaxClientError(`1 validation error detected: Value`,
          JSON.stringify(requestByTable),
           `at "requestItems" failed to satisfy constaint: Member must have length greater than or equal to 1`,
           DaxErrorCode.Validation, false);
      }

      stubData.keyValuesByTable = buffer.read();
      buffer.write(encoder.encodeMapStreamHeader());
      if(request.ReturnConsumedCapacity && request.ReturnConsumedCapacity !== 'NONE') {
        buffer.write(encoder.encodeInt(Constants.DaxDataRequestParam.ReturnConsumedCapacity));
        buffer.write(encoder.encodeInt(Constants.ReturnConsumedCapacityValues[request.ReturnConsumedCapacity]));
      }

      if(request.ReturnItemCollectionMetrics && request.ReturnItemCollectionMetrics !== 'NONE') {
        buffer.write(encoder.encodeInt(Constants.DaxDataRequestParam.ReturnItemCollectionMetrics));
        buffer.write(encoder.encodeInt(Constants.ReturnItemCollectionMetricsValue[request.ReturnItemCollectionMetrics]));
      }

      buffer.write(encoder.encodeStreamBreak());

      stubData.kwargs = buffer.read();
      request._keysPerTable = keysPerTable;
      request._attrListCache = this.attrListCache; // need cache for assemble, it's more efficient during that phase

      request._stubData = stubData;
      return stubData;
    });
  }

  write_batchWriteItem_116217951_1(data, tube) {
    tube.write(tube.cbor.encodeInt(1));
    tube.write(tube.cbor.encodeInt(116217951));
    tube.write(data.keyValuesByTable ? data.keyValuesByTable : tube.cbor.encodeNull());
    tube.write(data.kwargs ? data.kwargs : tube.cbor.encodeNull());
    tube.flush();
  }


  _validateWriteRequest(writeRequest) {
    if(writeRequest.PutRequest && writeRequest.DeleteRequest) {
      throw new DaxClientError('Both delete and put request cannot be set', DaxErrorCode.Validation, false);
    }
    if(!writeRequest.PutRequest && !writeRequest.DeleteRequest) {
      throw new DaxClientError('Both delete and put request cannot be empty', DaxErrorCode.Validation, false);
    }
  }

  _validateBatchWriteItem(attributes) {
    if(!attributes || Object.keys(attributes) === 0) {
      throw new DaxClientError(
        `1 validation error detected. Value ${JSON.stringify(attributes)} at "item" failed to satisfy constraint: Item must not be null`,
        DaxErrorCode.Validation, false);
    }

    Object.keys(attributes).forEach((name) => {
      if(this._simpleAttrValLength(attributes[name]) > BATCH_WRITE_MAX_ITEM_SIZE) {
        throw new DaxClientError('Item size has exceeded the maximum allowed size', DaxErrorCode.Validation, false);
      }
    });
  }

  _simpleAttrValLength(v) {
    if(!v) {
      return 0;
    } else if(v.S) {
      return v.S.length;
    } else if(v.B) {
      return v.B.length;
    } else if(v.N) {
      return v.N.length;
    } else if(v.BS) {
      let size = 0;
      for(let b of v.BS) {
        size += b.length;
      }
      return size;
    }
    // Only the primitive types are expected
    return 0;
  }
};

